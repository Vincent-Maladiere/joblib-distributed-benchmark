{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65e24ded-8a8f-48fd-bb8d-2c48e767361b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/15 18:33:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8947d36-210d-43cb-919d-f583ac5b6739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'JoblibSparkBackend'),\n",
       " ('spark.driver.host', '222-245.wifi-inria-saclay.saclay.inria.fr'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1676482422519'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '62530'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d73d892-7b01-4f3e-91ec-40a3b142c840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1585f528-d3e0-44c6-80ee-93fb7ae6baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib.parallel import register_parallel_backend\n",
    "from joblibspark.backend import SparkDistributedBackend\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "\n",
    "def _get_logger(name):\n",
    "    \"\"\" Gets a logger by name, or creates and configures it for the first time. \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # If the logger is configured, skip the configure\n",
    "    if not logger.handlers and not logging.getLogger().handlers:\n",
    "        handler = logging.StreamHandler(sys.stderr)\n",
    "        logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = _get_logger(\"joblib-spark\")\n",
    "\n",
    "\n",
    "class MySparkDistributedBackend(SparkDistributedBackend):\n",
    "    \n",
    "    # Hard cap on the number of concurrent hyperopt tasks (Spark jobs) to run. Set at 128.\n",
    "    MAX_CONCURRENT_JOBS_ALLOWED = 128\n",
    "    \n",
    "    def __init__(self, **backend_args):\n",
    "        super().__init__(**backend_args)\n",
    "        self._spark = spark\n",
    "    \n",
    "    @staticmethod\n",
    "    def _decide_parallelism(requested_parallelism,\n",
    "                            spark_default_parallelism,\n",
    "                            max_num_concurrent_tasks):\n",
    "        \"\"\"\n",
    "        Given the requested parallelism, return the max parallelism SparkTrials will actually use.\n",
    "        See the docstring for `parallelism` in the constructor for expected behavior.\n",
    "        \"\"\"\n",
    "        if max_num_concurrent_tasks == 0:\n",
    "            logger.warning(\n",
    "                \"The cluster has no executors currently. \"\n",
    "                \"The trials won't start until some new executors register.\"\n",
    "            )\n",
    "        if requested_parallelism is None:\n",
    "            parallelism = 1\n",
    "        elif requested_parallelism <= 0:\n",
    "            parallelism = max(spark_default_parallelism, max_num_concurrent_tasks, 1)\n",
    "            logger.warning(\n",
    "                \"Because the requested parallelism was None or a non-positive value, \"\n",
    "                \"parallelism will be set to ({d}), which is Spark's default parallelism ({s}), \"\n",
    "                \"or the current total of Spark task slots ({t}), or 1, whichever is greater. \"\n",
    "                \"We recommend setting parallelism explicitly to a positive value because \"\n",
    "                \"the total of Spark task slots is subject to cluster sizing.\".format(\n",
    "                    d=parallelism,\n",
    "                    s=spark_default_parallelism,\n",
    "                    t=max_num_concurrent_tasks,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            parallelism = requested_parallelism\n",
    "\n",
    "        if parallelism > MySparkDistributedBackend.MAX_CONCURRENT_JOBS_ALLOWED:\n",
    "            logger.warning(\n",
    "                \"Parallelism ({p}) is capped at SparkTrials.MAX_CONCURRENT_JOBS_ALLOWED ({c}).\"\n",
    "                .format(p=parallelism, c=MySparkDistributedBackend.MAX_CONCURRENT_JOBS_ALLOWED)\n",
    "            )\n",
    "            parallelism = MySparkDistributedBackend.MAX_CONCURRENT_JOBS_ALLOWED\n",
    "\n",
    "        if parallelism > max_num_concurrent_tasks:\n",
    "            logger.warning(\n",
    "                \"Parallelism ({p}) is greater than the current total of Spark task slots ({c}). \"\n",
    "                \"If dynamic allocation is enabled, you might see more executors allocated.\".format(\n",
    "                    p=requested_parallelism, c=max_num_concurrent_tasks\n",
    "                )\n",
    "            )\n",
    "        return parallelism\n",
    "    \n",
    "        \n",
    "    def effective_n_jobs(self, n_jobs):\n",
    "        \"\"\"\n",
    "        n_jobs is None will request 1 worker.\n",
    "        n_jobs=-1 means requesting all available workers,\n",
    "        but if cluster in dynamic allocation mode and available workers is zero\n",
    "        then use spark_default_parallelism and trigger spark worker dynamic allocation\n",
    "        \"\"\"\n",
    "        max_num_concurrent_tasks = self._get_max_num_concurrent_tasks()\n",
    "        spark_default_parallelism = self._spark.sparkContext.defaultParallelism\n",
    "        return self._decide_parallelism(\n",
    "            requested_parallelism=n_jobs,\n",
    "            spark_default_parallelism=spark_default_parallelism,\n",
    "            max_num_concurrent_tasks=max_num_concurrent_tasks,\n",
    "        )\n",
    "    \n",
    "register_parallel_backend(\"spark\", MySparkDistributedBackend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae96525b-4b3e-4a04-97d4-09f00d12979a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import cpu_count, Parallel, delayed, parallel_backend\n",
    "\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a69659c4-7cfd-43c4-b180-928652652a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/17 10:21:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel = Parallel(n_jobs=-1, backend=\"spark\")\n",
    "parallel._effective_n_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c4af7-2197-4bb3-8799-a245fb000973",
   "metadata": {},
   "source": [
    "# Joblib distributed\n",
    "\n",
    "This work was first introduced during a Joblib sprint organised at [Inria](https://team.inria.fr/soda/). Our aim is to benchmark the performances of joblib with various popular distributed backends, like Spark, Ray and Dask.\n",
    "\n",
    "Joblib is the package enabling embarassingly parallel operations in scikit-learn, powering estimators like `RandomForest` or `GridSearch`. Its main objective is to avoid oversubscription —spawning too much threads compared to the number of cores— by tricks like cgroup awareness, that is lacking in the `multiprocessing` package for exemple.\n",
    "\n",
    "When you run a jupyter notebook in your localhost, the number of physical cores of your machine should match the number of accessible cores by the software. However, when this jupyter notebook run within a Docker container from a Kubernetes pod, the number of physical cores of the cluster no longer match those accessible by your instance. This is where cgroup awareness is useful.\n",
    "\n",
    "In joblib you can access it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e7cfb13-280a-4a67-aef1-f62dc511e25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import cpu_count\n",
    "\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82987079-239d-4f1f-a5b0-e51e6b300809",
   "metadata": {},
   "outputs": [],
   "source": [
    "and with multiprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dcb703-ec10-4a16-ad9f-34a7098fa508",
   "metadata": {},
   "source": [
    "We begin our journey by defining a super simple function, that will simulate an expensive computation, and return the associated PID of the thread running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79251974-fb4d-40f8-9adf-88f31b176fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "def bench():\n",
    "    time.sleep(1)\n",
    "    return os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66182a67-3220-48ef-a4dc-e4b592b26716",
   "metadata": {},
   "outputs": [],
   "source": [
    "The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a0fc837-46cf-4e5a-b446-3eb2b3b3a332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    5.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({21872: 10, 21873: 10, 21874: 10, 21875: 10, 21876: 10, 21877: 10, 21878: 10, 21879: 10})\n",
      "CPU times: user 239 ms, sys: 101 ms, total: 339 ms\n",
      "Wall time: 11 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  80 out of  80 | elapsed:   10.9s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with parallel_backend('loky', n_jobs=8):\n",
    "    out = Parallel(verbose=1)(\n",
    "        delayed(bench)()\n",
    "        for i in range(80)\n",
    "    )\n",
    "    print(Counter(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "560fbc86-7e57-4204-91d0-851369d8a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblibspark import register_spark\n",
    "\n",
    "register_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86338fa-7bd4-467f-844c-557d486a88c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend SparkDistributedBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    5.4s                    \n",
      "[Stage 172:>  (0 + 1) / 1][Stage 173:>  (0 + 1) / 1][Stage 174:>  (0 + 1) / 1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({21729: 10, 21731: 10, 21732: 10, 21733: 10, 21727: 10, 21728: 10, 21730: 10, 21726: 10})\n",
      "CPU times: user 613 ms, sys: 246 ms, total: 859 ms\n",
      "Wall time: 10.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  80 out of  80 | elapsed:   10.7s finished           \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with parallel_backend('spark', n_jobs=8):\n",
    "    out = Parallel(verbose=1)(\n",
    "        delayed(bench)()\n",
    "        for i in range(80)\n",
    "    )\n",
    "    print(Counter(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03f1a2-5bb7-453f-9ed8-a83bfee94656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
